{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC 5611\n",
    "# Transformer Week 10 Submission\n",
    "Konrad Rozpadek\n",
    "\n",
    "4/3/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "torch.set_default_dtype(torch.float32)\n",
    "if torch.cuda.is_available():\n",
    "     torch.set_default_device(0)\n",
    "     print(\"Running on the GPU\")\n",
    "else:\n",
    "     print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bee20script.txt\", \"r\") as file:\n",
    "    data = file.read()\n",
    "    data = data.replace('\\n', ' ')\n",
    "    \n",
    "words = set(data.split(\" \"))\n",
    "word_dict = {}\n",
    "unique_words = len(words)\n",
    "for i, word in enumerate(words):\n",
    "    one_hot = torch.zeros(unique_words)\n",
    "    one_hot[i] = 1\n",
    "    word_dict[word] = one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoded = []\n",
    "for word in data.split(\" \"):\n",
    "    one_hot_encoded.append(word_dict[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2924"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_count = len(word_dict)\n",
    "token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(E):\n",
    "    num_tokens = E.size(0)\n",
    "    encoding = torch.zeros(num_tokens, d_model)\n",
    "    for pos in range(num_tokens):\n",
    "        for i in range(0,d_model,2):\n",
    "            encoding[pos, i] = math.sin(pos/(10000 ** ((2 * i) / d_model)))\n",
    "            encoding[pos, i + 1] = math.cos(pos/(10000 ** ((2 * i) / d_model)))\n",
    "    return encoding\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock():    \n",
    "    def __init__(self, d_model, d_k, d_v, d_f_f, h):\n",
    "        self.attention_scaling = 1/(math.sqrt(d_k))\n",
    "        self.W_O = torch.randn(h*d_v, d_model) * 0.1\n",
    "        self.W_O.requires_grad=True\n",
    "        #tuples with weights in order Q, K, V for each head\n",
    "        self.head_weights = []\n",
    "        for i in range(h):\n",
    "            W_Q = torch.randn(d_model, d_k) * 0.1\n",
    "            W_Q.requires_grad = True\n",
    "            W_K = torch.randn(d_model, d_k) * 0.1\n",
    "            W_K.requires_grad = True\n",
    "            W_V = torch.randn(d_model, d_v) * 0.1\n",
    "            W_V.requires_grad = True\n",
    "            self.head_weights.append((W_Q, W_K, W_V))\n",
    "        self.rms_norm = torch.nn.RMSNorm(d_model)\n",
    "        self.W_1 = torch.randn(d_model, d_f_f) * 0.1\n",
    "        self.W_1.requires_grad=True\n",
    "        self.W_2 = torch.randn(d_f_f, d_model) * 0.1\n",
    "        self.W_2.requires_grad = True\n",
    "        self.b_1 = torch.randn(1, d_f_f) * 0.1\n",
    "        self.b_1.requires_grad=True\n",
    "        self.b_2 = torch.randn(1, d_model) * 0.1\n",
    "        self.b_2.requires_grad = True\n",
    "        self.rms_norm = torch.nn.RMSNorm(d_model)\n",
    "\n",
    "\n",
    "    def multi_head_attention(self, E):\n",
    "        heads = []\n",
    "        for weights in self.head_weights:\n",
    "            Q_W = weights[0]\n",
    "            K_W = weights[1]\n",
    "            V_W = weights[2]\n",
    "\n",
    "            Q = E @ Q_W\n",
    "            K = E @ K_W\n",
    "            V = E @ V_W\n",
    "            heads.append(self.attention(Q, K, V))\n",
    "        return torch.cat(heads, dim=1) @ self.W_O\n",
    "    def attention(self, Q, K, V):\n",
    "        y_1 = Q @ K.t()\n",
    "        \n",
    "        y_2 = self.attention_scaling * y_1\n",
    "        \n",
    "        y_3 = self.attention_mask(y_2)\n",
    "        \n",
    "        max_y_3 = torch.max(y_3, 1, keepdim=True)[0]\n",
    "        exp_softmax = torch.exp(y_3-max_y_3)\n",
    "        sum_softmax = torch.sum(exp_softmax, 1, keepdim=True)\n",
    "        y_4 = exp_softmax/sum_softmax\n",
    "\n",
    "        y_5 = y_4 @ V\n",
    "        \n",
    "        return y_5\n",
    "    def feed_foward(self, input_):\n",
    "        linear_1 = input_ @ self.W_1 + self.b_1\n",
    "        relu = torch.max(torch.zeros(linear_1.size()), linear_1)\n",
    "        linear_2 = relu @ self.W_2 + self.b_2\n",
    "        return linear_2\n",
    "    def attention_mask(self, input_):\n",
    "        mask = torch.tril(input_, diagonal=0)\n",
    "        return mask.masked_fill(mask == 0, float('-inf'))\n",
    "    def add_and_norm(self, E, transformed_E):\n",
    "        add = E + transformed_E\n",
    "        return self.rms_norm(add)\n",
    "    def foward(self, E):\n",
    "        transformed_E = self.multi_head_attention(E)\n",
    "        normed_tran_E = self.add_and_norm(E, transformed_E)\n",
    "        feed_foward_E = self.feed_foward(normed_tran_E)\n",
    "        output = self.add_and_norm(normed_tran_E, feed_foward_E)\n",
    "        return output\n",
    "    def step(self, learning_rate):\n",
    "        for weights in self.head_weights:\n",
    "            Q_W = weights[0]\n",
    "            K_W = weights[1]\n",
    "            V_W = weights[2]\n",
    "\n",
    "            Q_W.data -= learning_rate * Q_W.grad\n",
    "            K_W.data -= learning_rate * K_W.grad\n",
    "            V_W.data -= learning_rate * V_W.grad\n",
    "            Q_W.grad.zero_()\n",
    "            K_W.grad.zero_()\n",
    "            V_W.grad.zero_()\n",
    "        self.W_1.data -= learning_rate * self.W_1.grad\n",
    "        self.W_2.data -= learning_rate * self.W_2.grad\n",
    "        self.b_1.data -= learning_rate * self.b_1.grad\n",
    "        self.b_2.data -= learning_rate * self.b_2.grad\n",
    "        self.W_1.grad.zero_()\n",
    "        self.W_2.grad.zero_()\n",
    "        self.b_1.grad.zero_()\n",
    "        self.b_2.grad.zero_()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "h = 4\n",
    "n = 6\n",
    "d_k = int(d_model / h)\n",
    "d_v = d_k\n",
    "d_f_f = 4*d_model\n",
    "num_epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "#Setup of weight inputs\n",
    "tran_blocks = []\n",
    "for i in range(n):\n",
    "        tran_blocks.append(TransformerBlock(d_model, d_k, d_v, d_f_f, h))\n",
    "W_E = torch.randn(token_count, d_model) * 0.1\n",
    "W_E.requires_grad=True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = torch.stack(one_hot_encoded[0:500])\n",
    "for i in range(num_epochs):\n",
    "    #Output Embedding Layer\n",
    "    embedding = input_ @ W_E \n",
    "\n",
    "    # Positional Encoding\n",
    "    embedding += positional_encoding(embedding)\n",
    "\n",
    "    # Decoder Block with n decoders\n",
    "    E_run = embedding\n",
    "    for tran_block in tran_blocks:\n",
    "        E_run = tran_block.foward(E_run)\n",
    "    tran_out = E_run\n",
    "\n",
    "    #Final Linear Layer reusing embedding matrix\n",
    "    final_linear = tran_out @ W_E.T\n",
    "\n",
    "    #Softmax\n",
    "    max_final = torch.max(final_linear, 1, keepdim=True)[0]\n",
    "    exp_softmax = torch.exp(final_linear-max_final)\n",
    "    sum_softmax = torch.sum(exp_softmax, 1, keepdim=True)\n",
    "    final_output = exp_softmax/sum_softmax\n",
    "\n",
    "    #Cross Entropy\n",
    "    cross_entropy = -1*((input_*torch.log(final_output + 1e-9)).sum())\n",
    "    cross_entropy.backward()\n",
    "    \n",
    "    #Stochastic gradient descent step\n",
    "    W_E.data -= learning_rate * W_E.grad\n",
    "    W_E.grad.zero_()\n",
    "    for tran_block in tran_blocks:\n",
    "        tran_block.step(learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test after some training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possession\n",
      "possession\n",
      "possession\n",
      "possession\n",
      "possession\n",
      "possession\n",
      "possession\n",
      "possession\n",
      "possession\n",
      "possession\n"
     ]
    }
   ],
   "source": [
    "def foward_tran(input_):\n",
    "    embedding = input_ @ W_E \n",
    "\n",
    "    embedding += positional_encoding(embedding)\n",
    "\n",
    "    E_run = embedding\n",
    "    for tran_block in tran_blocks:\n",
    "        E_run = tran_block.foward(E_run)\n",
    "\n",
    "    tran_out = E_run\n",
    "\n",
    "    final_linear = tran_out @ W_E.T\n",
    "\n",
    "    max_final = torch.max(final_linear, 1, keepdim=True)[0]\n",
    "    exp_softmax = torch.exp(final_linear-max_final)\n",
    "    sum_softmax = torch.sum(exp_softmax, 1, keepdim=True)\n",
    "    final_output = exp_softmax/sum_softmax\n",
    "    print(list(words)[torch.argmax(final_output[-1])])\n",
    "    return word_dict[list(words)[torch.argmax(final_output[-1])]]\n",
    "prompt = torch.stack([word_dict[\"black\"], word_dict[\"and\"], word_dict[\"yellow\"]])\n",
    "for i in range(10):\n",
    "    new_word = foward_tran(prompt)\n",
    "    prompt = torch.stack([*prompt,new_word])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
