# Overview
This project implements a decoder-only transformer model based on the architecture described in the paper "Attention is All You Need". The primary goal is to build the neural network layers from scratch, utilizing backpropagation equations that have been derived manually.  Only the mathematical operations in Pytorch were used like matrix multiplication, addition, and transpose in order to have good gpu support. Forwardpropagation and backpropagation were implemented from scratch in the layer.py file to create a reusable library to create the necessary layers for a transformer. The transformer_block.py file contains more complex layers composed such as Dot product attention which are composed of the simpler layers.

# Features

## Layers
- Input
- Linear
- MSELoss
- Regularization using Frobenius norm
- Softmax + Crossentropy
- Softmax
- Sum
- Transpose
- Concatenate
- Mask

## Transformer Components
- Transformer Block
- Multi-Head Attention
- Scaled Dot Product Attention
- Function for positional encoding

## Support for
- Neural networks that have a DAG architecture
- Transformers
- Variable batch size
- Hyperparameter tuning 
