{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "torch.set_default_dtype(torch.float32)\n",
    "if torch.cuda.is_available():\n",
    "     torch.set_default_device(0)\n",
    "     print(\"Running on the GPU\")\n",
    "else:\n",
    "     print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bee20script.txt\", \"r\") as file:\n",
    "    data = file.read()\n",
    "    data = data.replace('\\n', ' ')\n",
    "    \n",
    "words = set(data.split(\" \"))\n",
    "word_dict = {}\n",
    "unique_words = len(words)\n",
    "for i, word in enumerate(words):\n",
    "    one_hot = torch.zeros(unique_words)\n",
    "    one_hot[i] = 1\n",
    "    word_dict[word] = one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoded = []\n",
    "for word in data.split(\" \"):\n",
    "    one_hot_encoded.append(word_dict[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2924"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_count = len(word_dict)\n",
    "token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "h = 4\n",
    "n = 6\n",
    "d_k = int(d_model / h)\n",
    "d_v = d_k\n",
    "attention_scaling = 1/(math.sqrt(d_k))\n",
    "d_f_f = 4*d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2924])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ = torch.stack(one_hot_encoded[0:5])\n",
    "input_.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5349, -0.1775, -2.5538,  ...,  0.9493,  0.5778,  2.0673],\n",
       "        [ 1.3157, -1.3176,  1.0956,  ...,  0.4688,  0.1107, -0.2173],\n",
       "        [ 0.3226,  0.0986,  0.8342,  ...,  1.0390, -0.9386,  1.0450],\n",
       "        [ 1.5664, -0.7654, -0.0683,  ...,  0.2312, -0.7372, -0.3197],\n",
       "        [-0.0464,  0.7819,  0.5740,  ..., -0.5508, -1.5316,  1.0995]],\n",
       "       device='cuda:0', grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ = torch.stack(one_hot_encoded[0:5])\n",
    "W_E = torch.randn(token_count, d_model, requires_grad=True)\n",
    "embedding = input_ @ W_E \n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "          0.0000e+00,  1.0000e+00],\n",
       "        [ 8.4147e-01,  5.4030e-01,  8.0196e-01,  ...,  1.0000e+00,\n",
       "          1.0746e-08,  1.0000e+00],\n",
       "        [ 9.0930e-01, -4.1615e-01,  9.5814e-01,  ...,  1.0000e+00,\n",
       "          2.1492e-08,  1.0000e+00],\n",
       "        [ 1.4112e-01, -9.8999e-01,  3.4278e-01,  ...,  1.0000e+00,\n",
       "          3.2238e-08,  1.0000e+00],\n",
       "        [-7.5680e-01, -6.5364e-01, -5.4861e-01,  ...,  1.0000e+00,\n",
       "          4.2984e-08,  1.0000e+00]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def positional_encoding(E):\n",
    "    num_tokens = E.size(0)\n",
    "    encoding = torch.zeros(num_tokens, d_model)\n",
    "    for pos in range(num_tokens):\n",
    "        for i in range(0,d_model,2):\n",
    "            encoding[pos, i] = math.sin(pos/(10000 ** ((2 * i) / d_model)))\n",
    "            encoding[pos, i + 1] = math.cos(pos/(10000 ** ((2 * i) / d_model)))\n",
    "    return encoding\n",
    "positional_encoding(embedding)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding += positional_encoding(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_O = torch.randn(h*d_v, d_model, requires_grad=True)\n",
    "#tuples with weights in order Q, K, V for each head\n",
    "head_weights = []\n",
    "for i in range(h):\n",
    "    W_Q = torch.randn(d_model, d_k, requires_grad=True)\n",
    "    W_K = torch.randn(d_model, d_k, requires_grad=True)\n",
    "    W_V = torch.randn(d_model, d_v, requires_grad=True)\n",
    "    head_weights.append((W_Q, W_K, W_V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_mask(input_):\n",
    "    mask = torch.tril(input_, diagonal=0)\n",
    "    return mask.masked_fill(mask == 0, float('-inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(Q, K, V):\n",
    "    y_1 = Q @ K.t()\n",
    "    \n",
    "    y_2 = attention_scaling * y_1\n",
    "    \n",
    "    y_3 = attention_mask(y_2)\n",
    "    \n",
    "    max_y_3 = torch.max(y_3, 1, keepdim=True)[0]\n",
    "    exp_softmax = torch.exp(y_3-max_y_3)\n",
    "    sum_softmax = torch.sum(exp_softmax, 1, keepdim=True)\n",
    "    y_4 = exp_softmax/sum_softmax\n",
    "\n",
    "    y_5 = y_4 @ V\n",
    "    \n",
    "    return y_5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(E):\n",
    "    heads = []\n",
    "    for weights in head_weights:\n",
    "        Q_W = weights[0]\n",
    "        K_W = weights[1]\n",
    "        V_W = weights[2]\n",
    "\n",
    "        Q = E @ Q_W\n",
    "        K = E @ K_W\n",
    "        V = E @ V_W\n",
    "        heads.append(attention(Q, K, V))\n",
    "    return torch.cat(heads, dim=1) @ W_O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms_norm = torch.nn.RMSNorm(d_model)\n",
    "def add_and_norm(E, transformed_E):\n",
    "    add = E + transformed_E\n",
    "    return rms_norm(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8081,  0.2747,  1.3606,  ..., -1.4847, -1.5409, -0.8398],\n",
       "        [-1.4332,  0.9373,  0.4449,  ..., -0.4562, -1.4409, -3.1927],\n",
       "        [-1.5927,  0.7038,  1.7941,  ..., -1.4491, -0.9935, -1.5444],\n",
       "        [-0.3765, -0.7241,  0.9261,  ..., -1.7439, -1.3674, -1.8242],\n",
       "        [-0.1677,  0.4602,  0.5731,  ..., -0.1087, -2.5208, -2.2324]],\n",
       "       device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_and_norm(embedding, multi_head_attention(embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 512])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_and_norm(embedding, multi_head_attention(embedding)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_foward(input_, W_1, W_2, b_1, b_2):\n",
    "    linear_1 = input_ @ W_1 + b_1\n",
    "    relu = torch.max(torch.zeros(linear_1.size()), linear_1)\n",
    "    linear_2 = relu @ W_2 + b_2\n",
    "    return linear_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock():    \n",
    "    def __init__(self, d_model, d_k, d_v, d_f_f, h):\n",
    "        self.attention_scaling = 1/(math.sqrt(d_k))\n",
    "        self.W_O = torch.randn(h*d_v, d_model, requires_grad=True)\n",
    "        #tuples with weights in order Q, K, V for each head\n",
    "        self.head_weights = []\n",
    "        for i in range(h):\n",
    "            W_Q = torch.randn(d_model, d_k, requires_grad=True)\n",
    "            W_K = torch.randn(d_model, d_k, requires_grad=True)\n",
    "            W_V = torch.randn(d_model, d_v, requires_grad=True)\n",
    "            self.head_weights.append((W_Q, W_K, W_V))\n",
    "        self.rms_norm = torch.nn.RMSNorm(d_model)\n",
    "        self.W_1 = torch.randn(d_model, d_f_f, requires_grad=True)\n",
    "        self.W_2 = torch.randn(d_f_f, d_model, requires_grad=True)\n",
    "        self.b_1 = torch.randn(1, d_f_f, requires_grad=True)\n",
    "        self.b_2 = torch.randn(1, d_model, requires_grad=True)\n",
    "\n",
    "        self.rms_norm = torch.nn.RMSNorm(d_model)\n",
    "\n",
    "\n",
    "    def multi_head_attention(self, E):\n",
    "        heads = []\n",
    "        for weights in self.head_weights:\n",
    "            Q_W = weights[0]\n",
    "            K_W = weights[1]\n",
    "            V_W = weights[2]\n",
    "\n",
    "            Q = E @ Q_W\n",
    "            K = E @ K_W\n",
    "            V = E @ V_W\n",
    "            heads.append(self.attention(Q, K, V))\n",
    "        return torch.cat(heads, dim=1) @ self.W_O\n",
    "    def attention(self, Q, K, V):\n",
    "        y_1 = Q @ K.t()\n",
    "        \n",
    "        y_2 = self.attention_scaling * y_1\n",
    "        \n",
    "        y_3 = self.attention_mask(y_2)\n",
    "        \n",
    "        max_y_3 = torch.max(y_3, 1, keepdim=True)[0]\n",
    "        exp_softmax = torch.exp(y_3-max_y_3)\n",
    "        sum_softmax = torch.sum(exp_softmax, 1, keepdim=True)\n",
    "        y_4 = exp_softmax/sum_softmax\n",
    "\n",
    "        y_5 = y_4 @ V\n",
    "        \n",
    "        return y_5\n",
    "    def feed_foward(self, input_):\n",
    "        linear_1 = input_ @ self.W_1 + self.b_1\n",
    "        relu = torch.max(torch.zeros(linear_1.size()), linear_1)\n",
    "        linear_2 = relu @ self.W_2 + self.b_2\n",
    "        return linear_2\n",
    "    def attention_mask(self, input_):\n",
    "        mask = torch.tril(input_, diagonal=0)\n",
    "        return mask.masked_fill(mask == 0, float('-inf'))\n",
    "    def add_and_norm(self, E, transformed_E):\n",
    "        add = E + transformed_E\n",
    "        return self.rms_norm(add)\n",
    "    def foward(self, E):\n",
    "        transformed_E = self.multi_head_attention(E)\n",
    "        normed_tran_E = self.add_and_norm(E, transformed_E)\n",
    "        feed_foward_E = self.feed_foward(normed_tran_E)\n",
    "        output = self.add_and_norm(normed_tran_E, feed_foward_E)\n",
    "        return output\n",
    "    def step(learning_rate):\n",
    "        #redo with adam\n",
    "        for weights in self.head_weights:\n",
    "            Q_W = weights[0]\n",
    "            K_W = weights[1]\n",
    "            V_W = weights[2]\n",
    "\n",
    "            Q_W.data -= learning_rate * Q_W.grad\n",
    "            K_W.data -= learning_rate * K_W.grad\n",
    "            V_W.data -= learning_rate * V_W.grad\n",
    "            Q_W.grad.zero_()\n",
    "            K_W.grad.zero_()\n",
    "            V_W.grad.zero_()\n",
    "        self.W_1.data -= learning_rate * self.W_1.grad\n",
    "        self.W_2.data -= learning_rate * self.W_2.grad\n",
    "        self.b_1.data -= learning_rate * self.b_1.grad\n",
    "        self.b_2.data -= learning_rate * self.b_2.grad\n",
    "        self.W_1.grad.zero_()\n",
    "        self.W_2.grad.zero_()\n",
    "        self.b_1.grad.zero_()\n",
    "        self.b_2.grad.zero_()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 512])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tran_block = TransformerBlock(d_model, d_k, d_v, d_f_f, h)\n",
    "tran_block.foward(embedding).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9059,  0.5338, -2.3796,  ...,  0.5336,  0.2879, -0.4246],\n",
       "        [-0.8938,  0.7856, -2.0914,  ...,  0.6564,  0.1267, -0.3184],\n",
       "        [-0.8625,  0.6496, -1.9539,  ...,  0.4155,  0.0739, -0.3781],\n",
       "        [-0.7270,  0.8884, -1.9329,  ...,  0.5687, -0.1540, -0.2023],\n",
       "        [-0.7270,  0.8884, -1.9329,  ...,  0.5687, -0.1540, -0.2023]],\n",
       "       device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tran_blocks = []\n",
    "for i in range(n):\n",
    "    tran_blocks.append(TransformerBlock(d_model, d_k, d_v, d_f_f, h))\n",
    "\n",
    "E_run = embedding\n",
    "for tran_block in tran_blocks:\n",
    "    E_run = tran_block.foward(E_run)\n",
    "\n",
    "tran_out = E_run\n",
    "\n",
    "tran_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 512])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tran_out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_linear = tran_out @ W_E.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2924])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_linear.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_final = torch.max(final_linear, 1, keepdim=True)[0]\n",
    "exp_softmax = torch.exp(final_linear-max_final)\n",
    "sum_softmax = torch.sum(exp_softmax, 1, keepdim=True)\n",
    "final_output = exp_softmax/sum_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.5428e-35, 1.5197e-39, 2.0894e-25,  ..., 1.2045e-31, 2.0908e-28,\n",
       "         1.6687e-41],\n",
       "        [1.2802e-33, 5.1568e-43, 2.9224e-25,  ..., 7.5607e-30, 5.5299e-27,\n",
       "         1.3816e-40],\n",
       "        [2.6075e-35, 7.4269e-44, 1.1472e-25,  ..., 2.0649e-32, 1.1225e-27,\n",
       "         1.1467e-39],\n",
       "        [5.4939e-31, 4.2036e-41, 1.4182e-25,  ..., 2.2451e-31, 5.5102e-26,\n",
       "         1.8749e-36],\n",
       "        [5.4885e-31, 4.2082e-41, 1.4168e-25,  ..., 2.2438e-31, 5.4994e-26,\n",
       "         1.8757e-36]], device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "church.\n",
      "let's\n",
      "let's\n",
      "let's\n",
      "let's\n"
     ]
    }
   ],
   "source": [
    "output_vals = torch.argmax(final_output, axis = 1)\n",
    "for val in output_vals:\n",
    "    print(list(words)[val]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m     18\u001b[0m     embedding \u001b[39m=\u001b[39m input_ \u001b[39m@\u001b[39m W_E \n\u001b[0;32m---> 20\u001b[0m     embedding \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m positional_encoding(embedding)\n\u001b[1;32m     22\u001b[0m     E_run \u001b[39m=\u001b[39m embedding\n\u001b[1;32m     23\u001b[0m     \u001b[39mfor\u001b[39;00m tran_block \u001b[39min\u001b[39;00m tran_blocks:\n",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m, in \u001b[0;36mpositional_encoding\u001b[0;34m(E)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,d_model,\u001b[39m2\u001b[39m):\n\u001b[1;32m      6\u001b[0m         encoding[pos, i] \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39msin(pos\u001b[39m/\u001b[39m(\u001b[39m10000\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m ((\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m i) \u001b[39m/\u001b[39m d_model)))\n\u001b[0;32m----> 7\u001b[0m         encoding[pos, i \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m] \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39mcos(pos\u001b[39m/\u001b[39m(\u001b[39m10000\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m ((\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m i) \u001b[39m/\u001b[39m d_model)))\n\u001b[1;32m      8\u001b[0m \u001b[39mreturn\u001b[39;00m encoding\n",
      "File \u001b[0;32m/data/csc4611/conda-csc4611/lib/python3.12/site-packages/torch/utils/_device.py:75\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m     CURRENT_DEVICE \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mold_device\n\u001b[1;32m     73\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__exit__\u001b[39m(exc_type, exc_val, exc_tb)\n\u001b[0;32m---> 75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__torch_function__\u001b[39m(\u001b[39mself\u001b[39m, func, types, args\u001b[39m=\u001b[39m(), kwargs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     76\u001b[0m     kwargs \u001b[39m=\u001b[39m kwargs \u001b[39mor\u001b[39;00m {}\n\u001b[1;32m     77\u001b[0m     \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m _device_constructors() \u001b[39mand\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/data/csc4611/conda-csc4611/lib/python3.12/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_trace_dispatch_regular.py:377\u001b[0m, in \u001b[0;36mThreadTracer.__call__\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m event \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcall\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m NO_FTRACE\n\u001b[1;32m    375\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    376\u001b[0m     \u001b[39m# When stepping we can't take into account caching based on the breakpoints (only global filtering).\u001b[39;00m\n\u001b[0;32m--> 377\u001b[0m     \u001b[39mif\u001b[39;00m cache_skips\u001b[39m.\u001b[39;49mget(frame_cache_key) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    379\u001b[0m         \u001b[39mif\u001b[39;00m additional_info\u001b[39m.\u001b[39mpydev_original_step_cmd \u001b[39min\u001b[39;00m (CMD_STEP_INTO, CMD_STEP_INTO_MY_CODE) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _global_notify_skipped_step_in:\n\u001b[1;32m    380\u001b[0m             notify_skipped_step_in_because_of_filters(py_db, frame)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "h = 4\n",
    "n = 6\n",
    "d_k = int(d_model / h)\n",
    "d_v = d_k\n",
    "attention_scaling = 1/(math.sqrt(d_k))\n",
    "d_f_f = 4*d_model\n",
    "num_epochs = 10\n",
    "learning_rate = 0.01\n",
    "input_ = torch.stack(one_hot_encoded[0:500])\n",
    "\n",
    "tran_blocks = []\n",
    "for i in range(n):\n",
    "        tran_blocks.append(TransformerBlock(d_model, d_k, d_v, d_f_f, h))\n",
    "W_E = torch.randn(token_count, d_model, requires_grad=True)\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    embedding = input_ @ W_E \n",
    "\n",
    "    embedding += positional_encoding(embedding)\n",
    "\n",
    "    E_run = embedding\n",
    "    for tran_block in tran_blocks:\n",
    "        E_run = tran_block.foward(E_run)\n",
    "\n",
    "    tran_out = E_run\n",
    "\n",
    "    final_linear = tran_out @ W_E.T\n",
    "\n",
    "    max_final = torch.max(final_linear, 1, keepdim=True)[0]\n",
    "    exp_softmax = torch.exp(final_linear-max_final)\n",
    "    sum_softmax = torch.sum(exp_softmax, 1, keepdim=True)\n",
    "    final_output = exp_softmax/sum_softmax\n",
    "    cross_entropy = -1*((input_*torch.log(final_output + 1e-9)).sum())\n",
    "    cross_entropy.backward()\n",
    "    W_E.data -= learning_rate * W_E.grad\n",
    "    W_E.grad.zero_()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aviation,\n",
      "aviation,\n",
      "aviation,\n",
      "aviation,\n",
      "aviation,\n",
      "aviation,\n",
      "aviation,\n",
      "aviation,\n",
      "aviation,\n",
      "aviation,\n"
     ]
    }
   ],
   "source": [
    "def foward_tran(input_):\n",
    "    embedding = input_ @ W_E \n",
    "\n",
    "    embedding += positional_encoding(embedding)\n",
    "\n",
    "    E_run = embedding\n",
    "    for tran_block in tran_blocks:\n",
    "        E_run = tran_block.foward(E_run)\n",
    "\n",
    "    tran_out = E_run\n",
    "\n",
    "    final_linear = tran_out @ W_E.T\n",
    "\n",
    "    max_final = torch.max(final_linear, 1, keepdim=True)[0]\n",
    "    exp_softmax = torch.exp(final_linear-max_final)\n",
    "    sum_softmax = torch.sum(exp_softmax, 1, keepdim=True)\n",
    "    final_output = exp_softmax/sum_softmax\n",
    "    print(list(words)[torch.argmax(final_output[-1])])\n",
    "    return word_dict[list(words)[torch.argmax(final_output[-1])]]\n",
    "prompt = torch.stack([word_dict[\"black\"], word_dict[\"and\"], word_dict[\"yellow\"]])\n",
    "for i in range(10):\n",
    "    new_word = foward_tran(prompt)\n",
    "    prompt = torch.stack([*prompt,new_word])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
